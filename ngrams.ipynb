{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI4A-T0z5AU6"
      },
      "source": [
        "# Assignment 1\n",
        "You should submit the **UniversityNumber.ipynb** file and your final prediction file **UniversityNumber.test.out** to moodle. Make sure your code does not use your local files and that the results are reproducible. Before submitting, please **run your notebook and keep all running logs** so that we can check."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEomoMzH5Nf6"
      },
      "source": [
        "## 1 $n$-gram Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsSAtTqt7Q8a",
        "outputId": "3cc1db1e-34a8-45a4-99d1-c08dd57cc2e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-08 17:27:51--  https://raw.githubusercontent.com/ranpox/comp7607-fall2022/main/assignments/A1/data/lm/train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11185077 (11M) [text/plain]\n",
            "Saving to: ‘train.txt’\n",
            "\n",
            "train.txt           100%[===================>]  10.67M  46.8MB/s    in 0.2s    \n",
            "\n",
            "2022-10-08 17:27:52 (46.8 MB/s) - ‘train.txt’ saved [11185077/11185077]\n",
            "\n",
            "--2022-10-08 17:27:52--  https://raw.githubusercontent.com/ranpox/comp7607-fall2022/main/assignments/A1/data/lm/dev.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1385157 (1.3M) [text/plain]\n",
            "Saving to: ‘dev.txt’\n",
            "\n",
            "dev.txt             100%[===================>]   1.32M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-10-08 17:27:53 (10.4 MB/s) - ‘dev.txt’ saved [1385157/1385157]\n",
            "\n",
            "--2022-10-08 17:27:53--  https://raw.githubusercontent.com/ranpox/comp7607-fall2022/main/assignments/A1/data/lm/test.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1398013 (1.3M) [text/plain]\n",
            "Saving to: ‘test.txt’\n",
            "\n",
            "test.txt            100%[===================>]   1.33M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-10-08 17:27:53 (10.5 MB/s) - ‘test.txt’ saved [1398013/1398013]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O train.txt https://raw.githubusercontent.com/ranpox/comp7607-fall2022/main/assignments/A1/data/lm/train.txt\n",
        "!wget -O dev.txt https://raw.githubusercontent.com/ranpox/comp7607-fall2022/main/assignments/A1/data/lm/dev.txt\n",
        "!wget -O test.txt https://raw.githubusercontent.com/ranpox/comp7607-fall2022/main/assignments/A1/data/lm/test.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ElrINWW7oF7"
      },
      "source": [
        "### 1.1 Building vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eawcuVV19kZm"
      },
      "source": [
        "#### Code"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "contraction_mapping = {\n",
        "\"ain't\": \"is not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he'll've\": \"he he will have\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how is\",\n",
        "\"I'd\": \"I would\",\n",
        "\"I'd've\": \"I would have\",\n",
        "\"I'll\": \"I will\",\n",
        "\"I'll've\": \"I will have\",\n",
        "\"I'm\": \"I am\",\n",
        "\"I've\": \"I have\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'd've\": \"i would have\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'll've\": \"i will have\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it'll've\": \"it will have\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she'll've\": \"she will have\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"so's\": \"so as\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there would\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they'll've\": \"they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we would\",\n",
        "\"we'd've\": \"we would have\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we'll've\": \"we will have\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what'll've\": \"what will have\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"when's\": \"when is\",\n",
        "\"when've\": \"when have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"where've\": \"where have\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who'll've\": \"who will have\",\n",
        "\"who's\": \"who is\",\n",
        "\"who've\": \"who have\",\n",
        "\"why's\": \"why is\",\n",
        "\"why've\": \"why have\",\n",
        "\"will've\": \"will have\",\n",
        "\"won't\": \"will not\",\n",
        "\"won't've\": \"will not have\",\n",
        "\"would've\": \"would have\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"wouldn't've\": \"would not have\",\n",
        "\"y'all\": \"you all\",\n",
        "\"y'all'd\": \"you all would\",\n",
        "\"y'all'd've\": \"you all would have\",\n",
        "\"y'all're\": \"you all are\",\n",
        "\"y'all've\": \"you all have\",\n",
        "\"you'd\": \"you would\",\n",
        "\"you'd've\": \"you would have\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you'll've\": \"you will have\",\n",
        "\"you're\": \"you are\",\n",
        "\"you've\": \"you have\"\n",
        "}\n",
        "def expand_contractions(text, contraction_mapping):\n",
        "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),\n",
        "                                      flags=re.IGNORECASE|re.DOTALL)\n",
        "    def expand_match(contraction):\n",
        "        match = contraction.group(0)\n",
        "        first_char = match[0]\n",
        "        expanded_contraction = contraction_mapping.get(match)\\\n",
        "                                if contraction_mapping.get(match)\\\n",
        "                                else contraction_mapping.get(match.lower())                      \n",
        "        expanded_contraction = first_char+expanded_contraction[1:]\n",
        "        return expanded_contraction\n",
        "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
        "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
        "    return expanded_text\n",
        "\n",
        "#x=\"he's apple.we're happy\"\n",
        "#y=expand_contractions(x, contraction_mapping)\n",
        "#print(y)\n",
        "def clean(text):\n",
        "  r1 = u'[’!\"#$%&\\'()*+,-./:;=?@，。?★、…【】《》？“”‘’！[\\\\]^_`{|}~]+'\n",
        "  x=re.sub(r1,'',text)\n",
        "  return x\n"
      ],
      "metadata": {
        "id": "laqRahFqvLi6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-rNT_QL8Dvt",
        "outputId": "98f3bb23-14f8-46a6-fe50-ac6c76d2ae36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The vocabulary size is  22632\n"
          ]
        }
      ],
      "source": [
        "r1 = u'[a-zA-Z0-9<>’!\"#$%&\\'()*+,-./:;=?@，。?★、…【】《》？“”‘’！[\\\\]^_`{|}~]+'\n",
        "SOS = \"<s> \"\n",
        "EOS = \" </s>\"\n",
        "UNK = \"<UNK>\"\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.text import Text\n",
        "with open('train.txt','r') as f:\n",
        "  train=[l.strip() for l in f.readlines()]\n",
        "with open('test.txt','r') as f:\n",
        "  test=[l.strip() for l in f.readlines()]\n",
        "with open('dev.txt','r') as f:\n",
        "  dev=[l.strip() for l in f.readlines()]\n",
        "\n",
        "\n",
        "p_train=[]\n",
        "for each in train:\n",
        "  new=SOS+each+EOS\n",
        "  p_train.append(new)\n",
        "token=' '.join(p_train).split(' ')\n",
        "voc=nltk.FreqDist(token)\n",
        "token=[w if voc[w]>=3 else UNK for w in token]\n",
        "voc=nltk.FreqDist(token)\n",
        "print(\"The vocabulary size is \",len(voc))\n",
        "\n",
        "\n",
        "for each in train:\n",
        "  new=SOS+each+EOS\n",
        "  new=expand_contractions(new, contraction_mapping)\n",
        "  new=clean(new)\n",
        "  p_train.append(new)\n",
        "token=' '.join(p_train).split(' ')\n",
        "voc=nltk.FreqDist(token)\n",
        "token=[w if voc[w]>=3 else UNK for w in token]\n",
        "voc=nltk.FreqDist(token)\n",
        "\n",
        "\n",
        "p_dev=[]\n",
        "for each in dev:\n",
        "  new=SOS+each+EOS\n",
        "  new=expand_contractions(new, contraction_mapping)\n",
        "  new=clean(new)\n",
        "  p_dev.append(new)\n",
        "token_dev=' '.join(p_dev).split(' ')\n",
        "voc_dev=nltk.FreqDist(token_dev)\n",
        "token_dev=[w if voc[w]>=3 else UNK for w in token_dev]\n",
        "voc_dev=nltk.FreqDist(token_dev)\n",
        "\n",
        "p_test=[]\n",
        "for each in test:\n",
        "  new=SOS+each+EOS\n",
        "  new=expand_contractions(new, contraction_mapping)\n",
        "  new=clean(new)\n",
        "  p_test.append(new)\n",
        "token_test=' '.join(p_test).split(' ')\n",
        "voc_test=nltk.FreqDist(token_test)\n",
        "token_test=[w if voc[w]>=3 else UNK for w in token_test]\n",
        "voc_test=nltk.FreqDist(token_test)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7oBATsX8uHb"
      },
      "source": [
        "#### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU6VpAkS9odh"
      },
      "source": [
        "The vocabulary size is 22632.\n",
        "\n",
        "And the number of parameters of n-gram models shall be (22632)^n.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJ2BGUig8TqH"
      },
      "source": [
        "### 1.2 $n$-gram Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeyANMPe9ad_"
      },
      "source": [
        "#### Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7XN35PAEJ0E"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACSfNZGE8Yw2",
        "outputId": "7e1e0eeb-ae71-417d-b15a-de0cbea91212"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Perplexity of unigram on train set: 821.7608037311746\n",
            "For Perplexity of unigram on devset: 593.4345823562969\n",
            "For Perplexity of bigram on train set: 65.92105158201211\n",
            "For Perplexity of bigram on devset 36.780631801798116\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "unigram={}\n",
        "unigram_dev={}\n",
        "bigram={}\n",
        "bigram_dev={}\n",
        "trigram={}\n",
        "trigram_dev={}\n",
        "\n",
        "M=len(token)\n",
        "M_dev=len(token_dev)\n",
        "M_test=len(token_test)\n",
        "#unigram\n",
        "for w,c in voc.items():\n",
        "  unigram[w]=c/len(token)\n",
        "for w,c in voc_dev.items():\n",
        "  unigram_dev[w]=c/len(token_dev)\n",
        "\n",
        "prob_uni=[]\n",
        "prob_uni_dev=[]\n",
        "for each in token:\n",
        "  prob_uni.append(unigram[each])\n",
        "ppl_uni=pow(2,(-1/M) * sum(map(math.log2, prob_uni)))\n",
        "print(\"For Perplexity of unigram on train set:\",ppl_uni)\n",
        "\n",
        "for each in token_dev:\n",
        "  prob_uni_dev.append(unigram_dev[each])\n",
        "ppl_uni_dev=pow(2,(-1/M_dev) * sum(map(math.log2, prob_uni_dev)))\n",
        "print(\"For Perplexity of unigram on devset:\",ppl_uni_dev)\n",
        "\n",
        "#bigram\n",
        "bitoken=nltk.ngrams(token,2)\n",
        "bitoken_dev=nltk.ngrams(token_dev,2)\n",
        "bi_voc=nltk.FreqDist(bitoken)\n",
        "bi_voc_dev=nltk.FreqDist(bitoken_dev)\n",
        "for w,c in bi_voc.items():\n",
        "  uni_w=w[0]\n",
        "  bigram[w]=c/voc[uni_w]\n",
        "#print(bigram.items())\n",
        "prob=[]\n",
        "for each in nltk.ngrams(token,2):\n",
        "  prob.append(bigram[each])\n",
        "ppl=pow(2,(-1/M) * sum(map(math.log2, prob)))\n",
        "print(\"For Perplexity of bigram on train set:\",ppl)\n",
        "\n",
        "\n",
        "bitoken_dev=nltk.ngrams(token_dev,2)\n",
        "bi_voc_dev=nltk.FreqDist(bitoken_dev)\n",
        "for w,c in bi_voc_dev.items():\n",
        "  uni_w_dev=w[0]\n",
        "  bigram_dev[w]=c/voc_dev[uni_w_dev]\n",
        "\n",
        "\n",
        "prob_dev=[]\n",
        "for each in nltk.ngrams(token_dev,2):\n",
        "  prob_dev.append(bigram_dev[each])\n",
        "\n",
        "\n",
        "ppl_dev=pow(2,(-1/M_dev) * sum(map(math.log2, prob_dev)))\n",
        "print(\"For Perplexity of bigram on devset\",ppl_dev)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRWC56a19TbY"
      },
      "source": [
        "#### Discussion\n",
        "For Perplexity of unigram on train set: 627.0397515818853\n",
        "\n",
        "For Perplexity of unigram on devset: 569.0899883300671\n",
        "\n",
        "For Perplexity of bigram on train set: 62.2990399975297\n",
        "\n",
        "For Perplexity of bigram on devset 36.93644787950526"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuLL8CH1Ua-3"
      },
      "source": [
        "### 1.3 Smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7mWQhaCUixZ"
      },
      "source": [
        "#### 1.3.1 Add-one (Laplace) smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbbHxLDmVrz6"
      },
      "source": [
        "##### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "93_yLu9dVr0C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cec66155-0a56-4641-a008-b6fcaa9b84c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of train set with Add-one smoothing: 662.1751681146868\n",
            "Perplexity of dev set with Add-one smoothing: 1203.6144657098368\n"
          ]
        }
      ],
      "source": [
        "laplace=1  #add one\n",
        "\n",
        "for w,c in bi_voc.items():\n",
        "  uni_w=w[0]\n",
        "  bigram[w]=(c+laplace)/(voc[uni_w]+laplace*len(voc))\n",
        "prob=[]\n",
        "for each in nltk.ngrams(token,2):\n",
        "  prob.append(bigram[each])\n",
        "ppl=pow(2,(-1/M) * sum(map(math.log2, prob)))\n",
        "print(\"Perplexity of train set with Add-one smoothing:\",ppl)\n",
        "\n",
        "for w,c in bi_voc_dev.items():\n",
        "  uni_w_dev=w[0]\n",
        "  bigram_dev[w]=(c+laplace)/(voc_dev[uni_w_dev]+laplace*len(voc_dev))\n",
        "prob_dev=[]\n",
        "for each in nltk.ngrams(token_dev,2):\n",
        "  prob_dev.append(bigram_dev[each])\n",
        "ppl_dev=pow(2,(-1/M_dev) * sum(map(math.log2, prob_dev)))\n",
        "print(\"Perplexity of dev set with Add-one smoothing:\",ppl_dev)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8y1WOQtsVr0D"
      },
      "source": [
        "##### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTknh9pRVr0D"
      },
      "source": [
        "Perplexity of train set with Add-one smoothing: 699.9511398606322\n",
        "\n",
        "Perplexity of dev set with Add-one smoothing: 1102.8065795475807\n",
        "\n",
        "We can see the bi-gram model with smoothing will have higher perplexity on both train set and dev set compare to the bigram without smoothing, which means the smoothing will make the preplexity of the model higher."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTC0qJE8VVha"
      },
      "source": [
        "##### Optional: Add-k smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3itGMOOVuNg"
      },
      "source": [
        "###### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jhcuJWo7VuNg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "418cd15f-480e-47b4-e4fb-a7951dea745c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of train set with Add-k smoothing k= 0.5 : 453.6810165125592\n",
            "Perplexity of dev set with Add-k smoothing k= 0.5 : 792.16447828929\n",
            "Perplexity of train set with Add-k smoothing k= 0.05 : 160.86612876767887\n",
            "Perplexity of dev set with Add-k smoothing k= 0.05 : 207.41924813584\n",
            "Perplexity of train set with Add-k smoothing k= 0.01 : 101.46576742126888\n",
            "Perplexity of dev set with Add-k smoothing k= 0.01 : 98.98415081349003\n"
          ]
        }
      ],
      "source": [
        "lap_set=[0.5,0.05,0.01]\n",
        "\n",
        "for laplace in lap_set:\n",
        "  for w,c in bi_voc.items():\n",
        "    uni_w=w[0]\n",
        "    bigram[w]=(c+laplace)/(voc[uni_w]+laplace*len(voc))\n",
        "  prob=[]\n",
        "  for each in nltk.ngrams(token,2):\n",
        "    prob.append(bigram[each])\n",
        "  ppl=pow(2,(-1/M) * sum(map(math.log2, prob)))\n",
        "  print(\"Perplexity of train set with Add-k smoothing k=\",laplace,\":\",ppl)\n",
        "\n",
        "  for w,c in bi_voc_dev.items():\n",
        "    uni_w_dev=w[0]\n",
        "    bigram_dev[w]=(c+laplace)/(voc_dev[uni_w_dev]+laplace*len(voc_dev))\n",
        "  prob_dev=[]\n",
        "  for each in nltk.ngrams(token_dev,2):\n",
        "    prob_dev.append(bigram_dev[each])\n",
        "  ppl_dev=pow(2,(-1/M_dev) * sum(map(math.log2, prob_dev)))\n",
        "  print(\"Perplexity of dev set with Add-k smoothing k=\",laplace,\":\",ppl_dev)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzpU_p6CVuNg"
      },
      "source": [
        "###### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare to the bigram model with add-one smoothing, the add-k smoothing with k=0.5, 0.05, 0.01 has lower perplexity. And it shows these lower k will improve the performance of the model compare to add-one smoothing."
      ],
      "metadata": {
        "id": "5MjrFw5-6eEn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJFWxsN-Uj0Y"
      },
      "source": [
        "#### 1.3.2 Linear Interpolation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk11EpboWVCH"
      },
      "source": [
        "##### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "K4N_XuN6WVCQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "beaa7537-7a61-471d-978d-7736be93f6c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ppl of train set with linear Interpolation smoothing (hyperparameter:0.4,0.4,0.2): 6.076416941529217\n",
            "ppl of dev set with linear Interpolation smoothing:(hyperparameter:0.4,0.4,0.2): 8.154668457827047\n",
            "ppl of train set with linear Interpolation smoothing (hyperparameter:0.4,0.3,0.3): 5.054901596604651\n",
            "ppl of dev set with linear Interpolation smoothing:(hyperparameter:0.4,0.3,0.3): 8.624004360610448\n",
            "ppl of train set with linear Interpolation smoothing (hyperparameter:0.5,0.3,0.2): 5.602862832233529\n",
            "ppl of dev set with linear Interpolation smoothing:(hyperparameter:0.5,0.3,0.2): 7.155145927957223\n",
            "ppl of train set with linear Interpolation smoothing (hyperparameter:0.5,0.4,0.1): 7.26084446196351\n",
            "ppl of dev set with linear Interpolation smoothing:(hyperparameter:0.5,0.4,0.1): 6.819399207300997\n",
            "ppl of train set with linear Interpolation smoothing (hyperparameter:0.6,0.2,0.2): 5.224141309266024\n",
            "ppl of dev set with linear Interpolation smoothing:(hyperparameter:0.6,0.2,0.2): 6.404057069753345\n",
            "ppl of train set with linear Interpolation smoothing (hyperparameter:0.7,0.2,0.1): 6.2766677152011905\n",
            "ppl of dev set with linear Interpolation smoothing:(hyperparameter:0.7,0.2,0.1): 5.565662474791004\n",
            "ppl of train set with linear Interpolation smoothing (hyperparameter:0.8,0.1,0.1): 5.90447707789461\n",
            "ppl of dev set with linear Interpolation smoothing:(hyperparameter:0.8,0.1,0.1): 5.118143717537287\n",
            "best hyper parameters are [0.8, 0.1, 0.1]\n",
            "ppl of test set with linear Interpolation smoothing: 5.158632268777195\n"
          ]
        }
      ],
      "source": [
        "hp=[[0.4,0.4,0.2],[0.4,0.3,0.3],[0.5,0.3,0.2],[0.5,0.4,0.1],[0.6,0.2,0.2],[0.7,0.2,0.1],[0.8,0.1,0.1]]\n",
        "\n",
        "\n",
        "#trigram\n",
        "#train set\n",
        "tritoken=nltk.ngrams(token,3)\n",
        "tritoken_dev=nltk.ngrams(token_dev,3)\n",
        "tri_voc=nltk.FreqDist(tritoken)\n",
        "tri_voc_dev=nltk.FreqDist(tritoken_dev)\n",
        "best_hp=[]\n",
        "bestppl=9999999999\n",
        "for each in hp:\n",
        "  l1,l2,l3=each[0],each[1],each[2]\n",
        "  for w,c in tri_voc.items():\n",
        "    uni_w=w[1]\n",
        "    bi_w=w[:2]\n",
        "    trigram[w]=l1*c/bi_voc[bi_w]+l2*bi_voc[w[1:]]/voc[uni_w]+l3*voc[w[-1]]/len(voc)\n",
        "  #print(bigram.items())\n",
        "  prob=[]\n",
        "  for each in nltk.ngrams(token,3):\n",
        "    prob.append(trigram[each])\n",
        "\n",
        "  ppl=pow(2,(-1/M) * sum(map(math.log2, prob)))\n",
        "  print(\"ppl of train set with linear Interpolation smoothing (hyperparameter:%.1f,%.1f,%.1f):\"%(l1,l2,l3),ppl)\n",
        "\n",
        "  #dev set\n",
        "  for w,c in tri_voc_dev.items():\n",
        "    uni_w_dev=w[1]\n",
        "    bi_w_dev=w[:2]\n",
        "    trigram_dev[w]=l1*c/bi_voc_dev[bi_w_dev]+l2*bi_voc_dev[w[1:]]/voc_dev[uni_w_dev]\n",
        "    +l3*voc_dev[w[-1]]/len(voc_dev)\n",
        "  #print(bigram.items())\n",
        "  prob_dev=[]\n",
        "  for each in nltk.ngrams(token_dev,3):\n",
        "    prob_dev.append(trigram_dev[each])\n",
        "\n",
        "  ppl=pow(2,(-1/M_dev) * sum(map(math.log2, prob_dev)))\n",
        "  if ppl<bestppl:\n",
        "    bestppl=ppl\n",
        "    best_hp=[l1,l2,l3]\n",
        "  print(\"ppl of dev set with linear Interpolation smoothing:(hyperparameter:%.1f,%.1f,%.1f):\"%(l1,l2,l3),ppl)\n",
        "\n",
        "print(\"best hyper parameters are\",best_hp)\n",
        "#test set\n",
        "\n",
        "trigram_test={}\n",
        "[l1,l2,l3]=best_hp\n",
        "bitoken_test=nltk.ngrams(token_test,2)\n",
        "bi_voc_test=nltk.FreqDist(bitoken_test)\n",
        "tritoken_test=nltk.ngrams(token_test,3)\n",
        "tri_voc_test=nltk.FreqDist(tritoken_test)\n",
        "import math\n",
        "fix=math.pow(10,-100)\n",
        "for w,c in tri_voc_test.items():\n",
        "  uni_w_test=w[1]\n",
        "  bi_w_test=w[:2]\n",
        "  \n",
        "  trigram_test[w]=l1*c/bi_voc_test[bi_w_test]+l2*bi_voc_test[w[1:]]/voc_test[uni_w_test]\n",
        "  +l3*voc_test[w[-1]]/len(voc_test)\n",
        "#print(bigram.items())\n",
        "\n",
        "prob=[]\n",
        "for each in nltk.ngrams(token_test,3):\n",
        "  prob.append(trigram_test[each])\n",
        "ppl=pow(2,(-1/M_test) * sum(map(math.log2, prob)))\n",
        "print(\"ppl of test set with linear Interpolation smoothing:\",ppl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kh8v2P36WVCQ"
      },
      "source": [
        "##### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UnC5iF1IOL62"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvgTZcNFVato"
      },
      "source": [
        "##### Optional: Optimization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_hp=[]\n",
        "bestppl=9999999999\n",
        "for a in range(5,10):\n",
        "  for b in range(1,10):\n",
        "    for c in range(1,10):\n",
        "      for w,c in tri_voc_dev.items():\n",
        "        uni_w_dev=w[1]\n",
        "        bi_w_dev=w[:2]\n",
        "        trigram_dev[w]=(a/(a+b+c))*c/bi_voc_dev[bi_w_dev]+(b/(a+b+c))*bi_voc_dev[w[1:]]/voc_dev[uni_w_dev]\n",
        "        +(c/(a+b+c))*voc_dev[w[-1]]/len(voc_dev)\n",
        "      #print(bigram.items())\n",
        "      prob_dev=[]\n",
        "      for each in nltk.ngrams(token_dev,3):\n",
        "        prob_dev.append(trigram_dev[each])\n",
        "\n",
        "      ppl=pow(2,(-1/M_dev) * sum(map(math.log2, prob_dev)))\n",
        "      if ppl<bestppl:\n",
        "        bestppl=ppl\n",
        "        best_hp=[a/(a+b+c),b/(a+b+c),c/(a+b+c)]\n",
        "print(\"best hyper parameters are\",best_hp)\n",
        "print(\"ppl of dev set with linear Interpolation smoothing:(hyperparameter:%.5f,%.5f,%.5f):\"%(best_hp[0],best_hp[1],best_hp[2]),bestppl)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_t7p0CRTETBN",
        "outputId": "b53e8357-bd6f-4c92-9558-1e0be4e6a879"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best hyper parameters are [0.8181818181818182, 0.09090909090909091, 0.09090909090909091]\n",
            "ppl of dev set with linear Interpolation smoothing:(hyperparameter:0.81818,0.09091,0.09091): 8.330492818022256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKnXu98hWcfu"
      },
      "source": [
        "###### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKo4ZLASWcfu"
      },
      "source": [
        "There are many ways to find the optimal hyperparameters set, like Grid Search and Neural Network method. For example, we use Grid Search to  choose the best hyperparameter as above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E8xYXuCUoAR"
      },
      "source": [
        "## 2 Preposition Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "gewisxM5W5kQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10223970-6c79-465b-b6db-dd6ee1ac2275"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-08 17:54:07--  https://raw.githubusercontent.com/ranpox/comp7607-fall2022/main/assignments/A1/data/prep/dev.in\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 232043 (227K) [text/plain]\n",
            "Saving to: ‘dev.in’\n",
            "\n",
            "\rdev.in                0%[                    ]       0  --.-KB/s               \rdev.in              100%[===================>] 226.60K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2022-10-08 17:54:07 (3.57 MB/s) - ‘dev.in’ saved [232043/232043]\n",
            "\n",
            "--2022-10-08 17:54:07--  https://raw.githubusercontent.com/ranpox/comp7607-fall2022/main/assignments/A1/data/prep/dev.out\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8625 (8.4K) [text/plain]\n",
            "Saving to: ‘dev.out’\n",
            "\n",
            "dev.out             100%[===================>]   8.42K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-10-08 17:54:07 (65.4 MB/s) - ‘dev.out’ saved [8625/8625]\n",
            "\n",
            "--2022-10-08 17:54:07--  https://raw.githubusercontent.com/ranpox/comp7607-fall2022/main/assignments/A1/data/prep/test.in\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 235265 (230K) [text/plain]\n",
            "Saving to: ‘test.in’\n",
            "\n",
            "test.in             100%[===================>] 229.75K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2022-10-08 17:54:08 (3.52 MB/s) - ‘test.in’ saved [235265/235265]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O dev.in https://raw.githubusercontent.com/ranpox/comp7607-fall2022/main/assignments/A1/data/prep/dev.in\n",
        "!wget -O dev.out https://raw.githubusercontent.com/ranpox/comp7607-fall2022/main/assignments/A1/data/prep/dev.out\n",
        "!wget https://raw.githubusercontent.com/ranpox/comp7607-fall2022/main/assignments/A1/data/prep/test.in"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "\n",
        "with open('dev.in','r') as f:\n",
        "  devset=[l.strip() for l in f.readlines()]\n",
        "\n",
        "prep=['at','in','of','for','on']\n",
        "p_devset=[]\n",
        "for each in devset:\n",
        "  each=expand_contractions(each, contraction_mapping)\n",
        "  each=clean(each)\n",
        "  new=SOS+each+EOS\n",
        "  p_devset.append(new)\n",
        "token_pre=' '.join(p_devset).split(' ')\n",
        "#token_pre=word_tokenize(p_devset)\n",
        "#print(token_pre)\n",
        "voc_pre=nltk.FreqDist(token_pre)\n",
        "#token_pre=[w if voc_pre[w]>=3 else UNK for w in token_pre]\n",
        "voc_pre=nltk.FreqDist(token_pre)\n",
        "ans=[]\n",
        "#for w,c in trigram_dev.items():\n",
        " # print(w)\n",
        " # input()\n",
        "  \n",
        "for each in nltk.ngrams(token_pre,3):\n",
        "  if each[2]=='<PREP>':\n",
        "    prob=0\n",
        "    #p_out='of'\n",
        "    for p in prep:\n",
        "      temp=(each[0],each[1],p)\n",
        "      if temp in trigram_dev.keys():\n",
        "        if trigram_dev[temp]>prob:\n",
        "          prob=trigram_dev[temp]\n",
        "          p_out=p\n",
        "    if prob==0:\n",
        "      for p in prep:\n",
        "        if (each[1],p) in bigram_dev.keys():\n",
        "          if bigram_dev[(each[1],p)]>prob:\n",
        "            prob=bigram_dev[(each[1],p)]\n",
        "            p_out=p\n",
        "        #elif (each[1],p) in bigram_dev.keys():\n",
        "          #if bigram_dev[(each[1],p)]>prob:\n",
        "          #  prob=bigram_dev[(each[1],p)]\n",
        "           # p_out=p\n",
        "    if prob==0:\n",
        "      for p in prep:\n",
        "        if unigram_dev[p]>prob:\n",
        "          prob=unigram_dev[p]\n",
        "          p_out=p\n",
        "        \n",
        "    ans.append(p_out)\n",
        "with open('dev.out','r') as f:\n",
        "  devout=[l.strip() for l in f.readlines()]\n",
        "outtoken=' '.join(devout).split(' ')\n",
        "#print(len(outtoken))\n",
        "#print(len(ans))\n",
        "flag=0\n",
        "for i in range(len(outtoken)):\n",
        "  if outtoken[i]==ans[i]:\n",
        "    flag+=1\n",
        "\n",
        "acc=flag/len(outtoken)\n",
        "print(\"The accuracy on dev set is:\",acc)\n",
        "#print(ans[0:10])\n",
        "#print(outtoken[0:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbd5norUg4Vn",
        "outputId": "18836c75-271b-40e0-c9ad-ca470639dbad"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy on dev set is: 0.6462041409371595\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with open('test.in','r') as f:\n",
        "  testset=[l.strip() for l in f.readlines()]\n",
        "\n",
        "prep=['at','in','of','for','on']\n",
        "p_testset=[]\n",
        "for each in testset:\n",
        "  each=expand_contractions(each, contraction_mapping)\n",
        "  each=clean(each)\n",
        "  new=SOS+each+EOS\n",
        "  p_testset.append(new)\n",
        "testtoken_pre=' '.join(p_testset).split(' ')\n",
        "testvoc_pre=nltk.FreqDist(testtoken_pre)\n",
        "#testtoken_pre=[w if testvoc_pre[w]>=3 else UNK for w in testtoken_pre]\n",
        "testvoc_pre=nltk.FreqDist(testtoken_pre)\n",
        "testans=[]\n",
        "for each in nltk.ngrams(testtoken_pre,3):\n",
        "  if each[0]==\"<s>\":\n",
        "    testans.append([])\n",
        "  if each[2]=='<PREP>':\n",
        "    prob=0\n",
        "    #p_out='of'\n",
        "    for p in prep:\n",
        "      temp=(each[0],each[1],p)\n",
        "      if temp in trigram_dev.keys():\n",
        "        if trigram_dev[temp]>prob:\n",
        "          prob=trigram_dev[temp]\n",
        "          p_out=p\n",
        "    if prob==0:\n",
        "      for p in prep:\n",
        "        if (each[1],p) in bigram_dev.keys():\n",
        "          if bigram_dev[(each[1],p)]>prob:\n",
        "            prob=bigram_dev[(each[1],p)]\n",
        "            p_out=p\n",
        "        #elif (each[1],p) in bigram_dev.keys():\n",
        "          #if bigram_dev[(each[1],p)]>prob:\n",
        "          #  prob=bigram_dev[(each[1],p)]\n",
        "           # p_out=p\n",
        "    if prob==0:\n",
        "      for p in prep:\n",
        "        if unigram_dev[p]>prob:\n",
        "          prob=unigram_dev[p]\n",
        "          p_out=p\n",
        "        \n",
        "    testans[-1].append(p_out)\n",
        "\n",
        "#print(len(testans))\n",
        "print(testans)\n",
        "f=open(r'test.out','w')\n",
        "for each in testans:\n",
        "  print(\" \".join(str(i) for i in each), file=f)\n",
        "f.close\n",
        "print(len(testans))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOX8x5Z_xUQU",
        "outputId": "1c730132-b03c-419f-d748-d2deae2ecb94"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['in'], ['on'], ['in'], ['of', 'at'], ['for'], ['of'], ['of'], ['of'], ['of'], ['of'], ['of'], ['in', 'in', 'on', 'for'], ['in', 'in', 'in', 'for'], ['of', 'of', 'of', 'in'], ['for'], ['of'], ['in', 'in', 'at'], ['at'], ['of', 'of', 'on'], ['in'], ['of', 'in'], ['of', 'for'], ['of', 'of'], ['for'], ['of', 'of', 'in'], ['in', 'of', 'of'], ['in', 'of'], ['of', 'in', 'of', 'of'], ['at', 'of'], ['in', 'of'], ['in'], ['of', 'on', 'on'], ['in', 'of', 'in', 'of'], ['on', 'of', 'at'], ['in', 'of'], ['of', 'of'], ['of', 'of', 'of', 'of', 'of', 'in'], ['in'], ['in', 'in'], ['for', 'in', 'of', 'of'], ['of'], ['in'], ['of'], ['of', 'of'], ['of'], ['in'], ['of', 'of'], ['of', 'in'], ['of', 'of'], ['for', 'in'], ['in'], ['of'], ['of', 'of'], ['in', 'of', 'of', 'in'], ['of', 'of', 'of'], ['of', 'of'], ['on', 'of', 'of'], ['in'], ['of'], ['in'], ['of', 'in', 'of', 'of'], ['for', 'of', 'of', 'for', 'of'], ['of'], ['in', 'of', 'of'], ['of', 'of'], ['of', 'on', 'in'], ['at', 'of', 'of', 'in', 'of'], ['of', 'of'], ['in'], ['of', 'of'], ['of', 'in'], ['of', 'for', 'for'], ['in'], ['in', 'for', 'in'], ['of', 'of'], ['of', 'for', 'of'], ['for', 'on'], ['in'], ['of'], ['for'], ['of'], ['in', 'of', 'at', 'of'], ['in'], ['of', 'in'], ['of', 'of'], ['in', 'in', 'in', 'of', 'in', 'of'], ['of', 'of', 'of'], ['for', 'in'], ['of'], ['in', 'in', 'in', 'in', 'of'], ['of'], ['on'], ['of'], ['of'], ['for', 'on'], ['of', 'in', 'of'], ['of', 'of'], ['of', 'of'], ['of'], ['of'], ['of'], ['of', 'for', 'of'], ['on'], ['in'], ['of', 'in'], ['for'], ['in'], ['in'], ['of', 'in'], ['of'], ['in'], ['in', 'in', 'of'], ['of', 'of'], ['of'], ['in', 'of', 'of'], ['for'], ['of', 'of'], ['of', 'of'], ['in'], ['of'], ['at', 'of'], ['of', 'in', 'of', 'of'], ['in', 'of', 'of'], ['of'], ['of', 'of', 'of'], ['in'], ['in'], ['of'], ['for'], ['of'], ['of'], ['of'], ['of'], ['of'], ['of', 'of', 'in'], ['of', 'of'], ['of', 'of'], ['in', 'of'], ['in', 'for', 'in', 'of', 'in', 'of', 'of'], ['of'], ['of'], ['of'], ['of', 'of'], ['on'], ['of', 'for', 'of'], ['of'], ['of'], ['of'], ['of', 'of', 'of'], ['of'], ['in'], ['in', 'in'], ['of', 'in'], ['at'], ['on'], ['in', 'on', 'in', 'of'], ['of', 'on', 'of', 'of'], ['of'], ['in'], ['of', 'in'], ['of', 'of'], ['of', 'of'], ['of', 'of'], ['of'], ['of'], ['of'], ['of'], ['in', 'for'], ['of', 'of'], ['in'], ['in', 'in', 'of', 'in'], ['of'], ['on', 'in', 'in'], ['in'], ['of'], ['of', 'of', 'for'], ['of'], ['of'], ['in', 'of'], ['for', 'of'], ['of', 'in', 'of'], ['of', 'of'], ['in'], ['in', 'of'], ['of'], ['of', 'of', 'of'], ['in'], ['of', 'at'], ['of', 'of', 'of', 'of', 'in'], ['of', 'of'], ['of', 'in'], ['of'], ['in'], ['of', 'for'], ['of', 'for', 'in'], ['of'], ['of'], ['of', 'of', 'of'], ['of'], ['on', 'for'], ['of', 'on', 'in'], ['at'], ['of'], ['in'], ['of'], ['of', 'of', 'in', 'on', 'in'], ['of'], ['in', 'of', 'of'], ['in'], ['in', 'of', 'in', 'of', 'of'], ['of'], ['of'], ['on', 'in', 'of', 'of'], ['of'], ['in', 'of'], ['of'], ['of', 'in', 'of', 'of', 'of'], ['in'], ['of'], ['of', 'for'], ['of', 'of'], ['of'], ['of', 'of'], ['of', 'of', 'of', 'in'], ['of', 'of', 'on'], ['of'], ['of'], ['of', 'of'], ['of', 'of', 'of', 'for', 'of', 'of'], ['for'], ['of'], ['in'], ['of'], ['of', 'on'], ['of', 'of', 'of'], ['of', 'of'], ['for', 'on', 'of'], ['of', 'of'], ['in'], ['for'], ['of'], ['of', 'of'], ['in', 'in', 'of', 'of'], ['of'], ['of', 'of'], ['at', 'for', 'in'], ['in'], ['in'], ['of'], ['of'], ['of', 'of', 'in', 'of'], ['for', 'in'], ['of', 'of', 'of', 'of'], ['in', 'of', 'of', 'of'], ['of'], ['of'], ['of', 'of'], ['in', 'on'], ['of'], ['of'], ['of'], ['of', 'of', 'of', 'of', 'in', 'of'], ['in', 'in'], ['of', 'of', 'of'], ['for', 'of', 'of', 'for'], ['of'], ['for', 'in'], ['of'], ['for'], ['of', 'of'], ['in', 'of', 'of', 'of', 'of', 'in'], ['in'], ['of'], ['of', 'on'], ['of', 'of', 'for', 'of'], ['on'], ['in'], ['of', 'of'], ['of'], ['of'], ['of'], ['of', 'at', 'of', 'of', 'of'], ['in', 'on'], ['of'], ['in', 'on', 'of'], ['in', 'on'], ['of'], ['of', 'for', 'of', 'in', 'of'], ['of', 'on', 'of'], ['of'], ['of'], ['of'], ['on', 'of'], ['in', 'of', 'in'], ['in', 'of'], ['of'], ['on', 'of'], ['of'], ['at', 'in', 'in'], ['for', 'of'], ['in', 'of', 'of'], ['of'], ['of'], ['of'], ['of', 'in', 'for', 'of'], ['in'], ['of'], ['in', 'of', 'of', 'of'], ['in'], ['in', 'in', 'of'], ['of', 'of'], ['of'], ['of', 'of', 'at', 'of', 'of', 'of'], ['in'], ['of'], ['of', 'for', 'of', 'for'], ['of', 'for', 'of', 'in'], ['of'], ['of'], ['in'], ['of', 'of'], ['of'], ['of', 'of', 'of', 'for'], ['of', 'of'], ['of', 'of', 'of', 'of'], ['of'], ['of', 'of'], ['of', 'of', 'in'], ['of'], ['of', 'in'], ['for', 'of'], ['in', 'in', 'on', 'for'], ['of'], ['of'], ['of'], ['of', 'of'], ['of', 'of', 'of'], ['of', 'of'], ['in', 'of'], ['of'], ['of', 'in', 'in', 'of'], ['of', 'of', 'of'], ['of'], ['of', 'in'], ['at'], ['of', 'of', 'of'], ['for'], ['of'], ['of', 'for', 'of', 'in', 'in'], ['of'], ['of'], ['of', 'of', 'of', 'of', 'of', 'of', 'in'], ['in'], ['in', 'of', 'of'], ['of', 'of', 'of', 'of'], ['on'], ['in', 'of', 'in'], ['on'], ['of'], ['on', 'of'], ['of'], ['of', 'of'], ['of', 'of', 'of'], ['of'], ['in', 'of'], ['for', 'of'], ['of'], ['in'], ['in', 'in'], ['on', 'in'], ['in', 'of'], ['in', 'for'], ['on'], ['of', 'of', 'in'], ['of'], ['in'], ['of', 'for'], ['for'], ['of', 'on'], ['in', 'for'], ['in', 'of'], ['of'], ['of'], ['in', 'of', 'for'], ['of', 'in', 'in'], ['of', 'of'], ['of', 'of', 'on'], ['on', 'on', 'of'], ['of', 'of'], ['in', 'for', 'in'], ['in', 'of'], ['of', 'in'], ['of', 'in', 'of'], ['of'], ['at', 'for', 'in'], ['of', 'of', 'of'], ['at'], ['of'], ['in', 'for'], ['on'], ['of'], ['of', 'of', 'of', 'in'], ['of'], ['of'], ['of'], ['of', 'in', 'of'], ['of', 'on'], ['of', 'in', 'of', 'for'], ['of', 'in', 'in'], ['of'], ['for'], ['of'], ['for'], ['in', 'in', 'on'], ['of', 'for'], ['in'], ['of'], ['in', 'of', 'of'], ['of', 'in', 'of'], ['on', 'of'], ['of', 'of', 'of'], ['of'], ['of'], ['in', 'of'], ['for', 'of', 'in', 'of', 'in', 'of', 'for'], ['of', 'in', 'of'], ['in', 'in'], ['of', 'for', 'of', 'on'], ['in', 'of', 'on'], ['in', 'of', 'of'], ['in', 'in'], ['of'], ['of'], ['of', 'of', 'of', 'of', 'of', 'in'], ['for', 'of', 'of', 'of'], ['in', 'in', 'in', 'of'], ['of'], ['in', 'on', 'at'], ['of'], ['for', 'of'], ['of', 'of', 'in'], ['of'], ['of', 'of', 'of'], ['on', 'of'], ['in', 'on', 'in'], ['of', 'of'], ['on'], ['of'], ['for'], ['of', 'in'], ['of'], ['in', 'on'], ['of'], ['on'], ['of'], ['in'], ['in', 'on'], ['of'], ['of', 'on'], ['for'], ['of'], ['of'], ['in'], ['of'], ['of', 'of'], ['of'], ['of', 'in'], ['of'], ['of', 'of'], ['of', 'in'], ['of', 'at', 'of'], ['at'], ['at', 'of'], ['of'], ['in'], ['of'], ['of', 'of'], ['of', 'in', 'of'], ['in', 'in', 'of', 'of'], ['of'], ['of', 'in', 'on'], ['in'], ['of', 'in'], ['of'], ['of', 'of'], ['of'], ['in'], ['of'], ['in', 'of'], ['of'], ['of', 'of', 'of', 'of'], ['in'], ['of'], ['of'], ['of', 'of', 'on'], ['of', 'of', 'of', 'of'], ['in'], ['of', 'of', 'in'], ['in', 'for', 'in'], ['at'], ['of'], ['of', 'in', 'of'], ['of'], ['in'], ['of'], ['in', 'of', 'in'], ['in', 'at', 'in'], ['for', 'of', 'of'], ['of', 'in', 'in'], ['on'], ['of'], ['of', 'of'], ['of'], ['of', 'on', 'of'], ['of'], ['at'], ['of'], ['in', 'of'], ['of'], ['of', 'for', 'of'], ['for'], ['of', 'of', 'of', 'in', 'of', 'in'], ['of', 'of', 'of'], ['in', 'of', 'of', 'of', 'of'], ['of', 'of', 'of', 'in', 'of'], ['in'], ['of', 'of'], ['for'], ['of', 'of', 'of'], ['of'], ['in'], ['of', 'in'], ['of'], ['in'], ['of', 'for', 'for', 'for', 'of'], ['of', 'of', 'of', 'in'], ['of', 'of'], ['of'], ['in', 'of', 'of'], ['of', 'of'], ['of', 'of', 'at', 'of'], ['for'], ['for'], ['of'], ['of', 'in', 'of', 'on'], ['at', 'in', 'of', 'in'], ['in'], ['of'], ['for'], ['of'], ['for'], ['of', 'in'], ['of', 'of'], ['of', 'in'], ['of', 'in'], ['of'], ['of'], ['of', 'in', 'of'], ['in'], ['of', 'in', 'in'], ['in', 'of'], ['of', 'for'], ['in', 'of', 'in'], ['in', 'in'], ['in', 'of'], ['of', 'of', 'of'], ['of', 'of', 'of'], ['of'], ['of'], ['of'], ['of'], ['of', 'of'], ['of', 'of'], ['of', 'for'], ['of'], ['for', 'on'], ['for'], ['of', 'of'], ['at'], ['on', 'of', 'on', 'of', 'of'], ['of', 'of', 'in', 'in'], ['of', 'in'], ['on'], ['in'], ['for'], ['in'], ['in'], ['of', 'of'], ['of'], ['of', 'of'], ['of', 'of'], ['for', 'for'], ['of'], ['in', 'of'], ['in', 'of', 'of', 'of'], ['of'], ['of'], ['of'], ['of'], ['of', 'of', 'in'], ['in', 'of', 'of', 'in', 'of'], ['of', 'for', 'of'], ['in', 'of', 'of'], ['of', 'in', 'of'], ['of'], ['of'], ['of'], ['of', 'of'], ['in'], ['of', 'at', 'of'], ['of', 'of', 'in', 'of'], ['of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'for'], ['in', 'of', 'of', 'of'], ['of'], ['of'], ['on', 'of', 'of'], ['of', 'of'], ['of', 'of'], ['of', 'in', 'of'], ['in'], ['in'], ['in', 'of', 'in'], ['in'], ['of', 'in'], ['in'], ['of', 'of'], ['on'], ['in', 'in'], ['of'], ['for'], ['in'], ['of'], ['for', 'on', 'of'], ['of'], ['in', 'of'], ['in'], ['for'], ['of'], ['in', 'of', 'for'], ['on'], ['of'], ['in', 'of'], ['of', 'of', 'of'], ['of'], ['for', 'of', 'of'], ['for', 'for'], ['of'], ['in', 'of', 'on', 'in'], ['of', 'of', 'of', 'of'], ['of', 'in'], ['of', 'on'], ['in'], ['of'], ['on', 'in'], ['in', 'in'], ['in', 'in', 'in'], ['in', 'in', 'of'], ['of'], ['in', 'of'], ['in', 'of'], ['of'], ['of'], ['on'], ['of'], ['for', 'of'], ['of', 'of'], ['of', 'of'], ['for'], ['of', 'of'], ['of', 'of'], ['in', 'of', 'for', 'on'], ['of'], ['of'], ['of'], ['of', 'of', 'of'], ['for'], ['of'], ['of', 'of'], ['in'], ['of', 'on', 'of'], ['of', 'in'], ['of'], ['in', 'of', 'for', 'in', 'of'], ['of', 'on'], ['in', 'of'], ['of', 'of'], ['of'], ['of'], ['on', 'of'], ['in', 'of'], ['at', 'of'], ['in', 'of'], ['of'], ['of'], ['of', 'of'], ['of'], ['of'], ['of', 'for'], ['on'], ['in', 'of', 'of'], ['of'], ['of'], ['in', 'in'], ['in', 'of'], ['in'], ['of', 'of'], ['in', 'in', 'in', 'in'], ['in', 'of', 'in', 'in', 'of'], ['of', 'for'], ['in', 'in', 'of', 'in'], ['of'], ['of'], ['in', 'of'], ['of', 'of', 'in'], ['in', 'in', 'in'], ['of'], ['of', 'of', 'of', 'of'], ['in', 'for', 'of', 'in', 'on', 'of', 'in'], ['of', 'of'], ['at', 'for', 'in'], ['of', 'of', 'in'], ['at', 'in', 'of'], ['of', 'of', 'for'], ['of', 'of', 'for'], ['of'], ['for', 'of'], ['at', 'of', 'in'], ['of'], ['on', 'for', 'of'], ['of'], ['in'], ['of', 'of'], ['on', 'of'], ['in', 'in'], ['of', 'of'], ['of'], ['at', 'of'], ['for', 'of'], ['of', 'in'], ['of'], ['on'], ['on'], ['in', 'of'], ['in'], ['of'], ['in', 'in'], ['of', 'of'], ['of'], ['of', 'in'], ['of'], ['of', 'of'], ['for'], ['of', 'on', 'of'], ['in'], ['of'], ['of'], ['of'], ['of', 'in', 'of', 'of', 'in'], ['of', 'of'], ['of'], ['on', 'of'], ['in', 'of', 'in', 'of', 'of'], ['of'], ['in', 'of'], ['of', 'in'], ['of'], ['of', 'of', 'of'], ['for', 'for'], ['of', 'in'], ['for'], ['for', 'of', 'of'], ['of'], ['in'], ['of', 'of', 'of', 'in', 'in'], ['for', 'of', 'of'], ['on'], ['of', 'of'], ['in', 'of'], ['of'], ['in'], ['of', 'in'], ['for', 'in'], ['in', 'for', 'of', 'in'], ['of', 'of'], ['at'], ['in', 'of', 'of', 'in'], ['of'], ['of'], ['in', 'on', 'for', 'of'], ['in', 'of', 'in'], ['of'], ['of', 'of', 'on'], ['of', 'of', 'of', 'in', 'in'], ['of', 'of', 'of', 'of'], ['of', 'of', 'of'], ['at', 'in'], ['of'], ['for', 'in'], ['in'], ['of'], ['in'], ['of'], ['of', 'of'], ['in', 'in'], ['in'], ['of', 'on', 'in'], ['in'], ['on', 'of'], ['of', 'in', 'in', 'in'], ['of', 'in'], ['of', 'on'], ['of'], ['of'], ['of'], ['of'], ['of', 'in', 'of'], ['for', 'in'], ['for', 'in'], ['on', 'of', 'in', 'of'], ['in', 'of', 'of'], ['on'], ['of'], ['in', 'of', 'of'], ['of', 'for'], ['of'], ['in', 'of', 'of'], ['of', 'for'], ['of', 'of'], ['in'], ['of', 'in'], ['in'], ['in', 'on'], ['of', 'of'], ['of', 'of', 'in'], ['of'], ['of', 'of', 'of', 'of', 'of'], ['of', 'of'], ['of', 'in'], ['in', 'of', 'of'], ['of'], ['of'], ['of', 'in'], ['in'], ['of'], ['in'], ['of', 'for', 'in'], ['of', 'of', 'of'], ['of'], ['in', 'in', 'on', 'for'], ['in'], ['for'], ['of'], ['in'], ['for'], ['of', 'of'], ['in', 'of'], ['in', 'of', 'of', 'of'], ['of', 'on'], ['in'], ['of'], ['of'], ['of'], ['in', 'in'], ['in'], ['of'], ['of', 'on'], ['in', 'of'], ['in'], ['in'], ['of'], ['on'], ['of'], ['of', 'of'], ['in', 'in'], ['at', 'of', 'in'], ['of'], ['in'], ['in', 'of', 'in', 'of', 'of'], ['of', 'of'], ['on', 'in', 'in'], ['in', 'of', 'of'], ['of', 'in'], ['of'], ['for', 'of', 'of'], ['of', 'of', 'in', 'of'], ['of'], ['of', 'of', 'of', 'of'], ['of'], ['in'], ['of', 'of', 'for'], ['in', 'in'], ['of', 'of'], ['of'], ['in'], ['of', 'of'], ['for'], ['for', 'of'], ['for', 'of', 'of', 'of', 'for', 'in'], ['in'], ['for'], ['in', 'of', 'of'], ['of'], ['of', 'for', 'in', 'of'], ['of', 'of'], ['of'], ['of'], ['for', 'of', 'at'], ['of'], ['of'], ['on'], ['in', 'of', 'of', 'of'], ['in'], ['for', 'in'], ['for'], ['in', 'in', 'in'], ['on', 'of'], ['of'], ['of'], ['of', 'of', 'of'], ['of'], ['of', 'of'], ['for', 'of'], ['in', 'of', 'in', 'at'], ['at', 'of', 'of', 'of', 'of', 'on', 'of'], ['in', 'of'], ['of'], ['in'], ['of'], ['on', 'of'], ['of', 'of'], ['of'], ['of', 'for'], ['of'], ['of'], ['of'], ['of'], ['of', 'in'], ['in', 'of'], ['of', 'of', 'for', 'of', 'of'], ['in', 'of'], ['of'], ['of'], ['for', 'of'], ['in', 'of'], ['of'], ['in'], ['of'], ['of'], ['for'], ['of', 'of'], ['of', 'of', 'for'], ['in', 'of', 'of', 'of'], ['in', 'of'], ['in'], ['of', 'of', 'in'], ['for', 'of', 'of', 'of', 'of', 'of'], ['in', 'in'], ['of'], ['in', 'of', 'of', 'of'], ['of', 'of'], ['of'], ['in', 'in'], ['of'], ['of', 'of', 'of'], ['of', 'of', 'in', 'of'], ['of'], ['of', 'in'], ['in', 'of'], ['of', 'in', 'of'], ['in', 'in', 'of'], ['of', 'of'], ['of', 'for', 'at', 'of', 'of', 'of'], ['in'], ['on', 'for', 'of'], ['for', 'in'], ['of'], ['of', 'of', 'of', 'in'], ['at', 'in'], ['in', 'in'], ['for'], ['for'], ['in'], ['of', 'of', 'of', 'in', 'of'], ['for'], ['of'], ['in', 'of', 'of'], ['in'], ['in'], ['in'], ['in', 'of'], ['in'], ['in'], ['of'], ['of'], ['for'], ['of'], ['of', 'of', 'in', 'in'], ['of', 'of'], ['in', 'in'], ['of'], ['of'], ['for'], ['in', 'in'], ['of'], ['in', 'of', 'of', 'of'], ['on'], ['for', 'for', 'of', 'in'], ['on'], ['of', 'of'], ['of'], ['in'], ['in', 'for'], ['for', 'in'], ['of', 'of'], ['of'], ['in', 'for'], ['in', 'of', 'of', 'of'], ['of', 'of', 'of'], ['of'], ['in'], ['of'], ['of'], ['of', 'on', 'in', 'of'], ['in', 'of', 'of', 'of'], ['of', 'of', 'for', 'of'], ['in'], ['for'], ['in'], ['of'], ['of', 'on'], ['in', 'of'], ['on'], ['of', 'of', 'in'], ['for', 'of', 'in'], ['of'], ['of'], ['in'], ['in', 'of'], ['of', 'of'], ['of', 'of', 'of', 'of', 'in', 'of', 'of'], ['on'], ['of'], ['in', 'of'], ['of', 'of', 'of', 'in', 'in', 'of'], ['of', 'of'], ['for', 'of', 'of'], ['in', 'in'], ['of'], ['of'], ['in', 'in'], ['of', 'on', 'for', 'of'], ['of', 'in', 'in'], ['in'], ['of', 'in'], ['of', 'of', 'of', 'in'], ['of', 'in', 'of'], ['in'], ['of'], ['in'], ['of'], ['of'], ['of'], ['of', 'in'], ['of'], ['of', 'of'], ['of'], ['of'], ['on', 'in', 'of'], ['for'], ['of', 'for'], ['for'], ['of', 'for'], ['of', 'of', 'in', 'of', 'in', 'of', 'of'], ['for'], ['of'], ['of'], ['of', 'of'], ['of', 'of'], ['of', 'of'], ['of', 'for'], ['of', 'of'], ['of', 'of', 'for'], ['of', 'of'], ['in'], ['in', 'of'], ['in'], ['of', 'of'], ['at', 'for', 'in'], ['of', 'of'], ['of', 'in', 'in'], ['of'], ['in', 'in', 'of', 'of', 'of'], ['in'], ['in', 'in'], ['of'], ['in', 'of'], ['of', 'of'], ['of', 'of', 'of', 'of', 'of'], ['of', 'of'], ['of', 'in', 'of', 'of'], ['of', 'in'], ['for', 'of', 'of'], ['of'], ['in'], ['of'], ['of'], ['in'], ['of', 'on'], ['of', 'of'], ['of'], ['on', 'of'], ['of'], ['in'], ['of', 'for', 'of'], ['on', 'for', 'for', 'of'], ['in'], ['of'], ['in'], ['of'], ['of'], ['in', 'in'], ['in'], ['of', 'of'], ['of'], ['of', 'of'], ['of'], ['of'], ['in', 'of'], ['in'], ['of', 'for'], ['of', 'in', 'of'], ['for', 'of'], ['in'], ['of', 'of', 'of', 'of', 'of'], ['in', 'in'], ['of', 'in', 'of'], ['of', 'of', 'in', 'in'], ['of', 'of', 'of', 'in'], ['on', 'in'], ['on', 'of', 'of'], ['in', 'in', 'in'], ['in'], ['of', 'of'], ['of', 'in'], ['of'], ['in'], ['of'], ['of'], ['of', 'in'], ['of', 'of', 'of'], ['of'], ['in'], ['of'], ['in', 'in', 'of'], ['of'], ['of'], ['of'], ['of', 'in'], ['for'], ['of', 'of'], ['of'], ['of', 'of'], ['for', 'of'], ['in'], ['in'], ['of'], ['of', 'of', 'of'], ['of', 'of', 'of'], ['of', 'for'], ['of', 'of', 'of'], ['in', 'in', 'at'], ['of', 'in', 'in'], ['in'], ['in'], ['in', 'on', 'of'], ['for'], ['of', 'of', 'for'], ['of', 'of', 'in', 'in', 'of'], ['of', 'of'], ['of', 'of', 'for', 'of', 'for'], ['of'], ['at', 'in'], ['of', 'of', 'of', 'of'], ['in'], ['of', 'on'], ['of'], ['in'], ['for', 'of'], ['of'], ['in', 'in'], ['of', 'of'], ['in', 'of'], ['of', 'of'], ['of'], ['of', 'of'], ['of', 'of'], ['of'], ['of', 'of', 'in'], ['of', 'of', 'for'], ['of'], ['in', 'for', 'in', 'in', 'of'], ['in', 'of', 'in', 'on', 'for'], ['of', 'of'], ['of', 'of'], ['of'], ['of'], ['for', 'of'], ['for'], ['of'], ['of', 'of', 'of', 'of'], ['of'], ['of', 'of', 'of', 'of', 'in'], ['of', 'in', 'of', 'for'], ['of', 'of'], ['of', 'in'], ['of'], ['of'], ['in'], ['in'], ['of', 'for', 'in'], ['on', 'of'], ['at'], ['at'], ['in', 'in'], ['in'], ['of', 'of'], ['in', 'of'], ['of', 'of', 'for'], ['of'], ['of', 'of', 'in', 'of'], ['of', 'in'], ['on', 'in', 'of'], ['of', 'in', 'of'], ['on', 'of'], ['of'], ['of'], ['in', 'for', 'of', 'of'], ['of', 'for', 'of'], ['for', 'in'], ['at'], ['of', 'of'], ['of', 'of', 'on'], ['of', 'of'], ['in'], ['in'], ['of'], ['of', 'of'], ['of', 'of'], ['of', 'of'], ['in', 'in', 'of'], ['of', 'of'], ['for', 'of'], ['in'], ['for', 'of'], ['in', 'in'], ['of'], ['of'], ['of'], ['of'], ['on', 'of', 'of'], ['of', 'of'], ['in'], ['at', 'for'], ['of'], ['of', 'on', 'in'], ['in', 'in'], ['at', 'of', 'of'], ['at', 'of', 'in'], ['in'], ['of'], ['in', 'of', 'in', 'of', 'of'], ['of', 'in'], ['for'], ['of'], ['of', 'in', 'in', 'for', 'of'], ['on'], ['of', 'of', 'of'], ['of'], ['of', 'of', 'of', 'of'], ['in', 'of'], ['of'], ['in'], ['for'], ['of', 'in'], ['of'], ['of', 'of', 'of'], ['on', 'of', 'on'], ['of', 'of'], ['of', 'in'], ['in'], ['in', 'in'], ['of', 'in'], ['of'], ['of'], ['in', 'of', 'in'], ['of'], ['in'], ['on'], ['of'], ['on'], ['of', 'of'], ['of'], ['in'], ['at'], ['of'], ['of'], ['in', 'on', 'in'], ['of', 'of'], ['in'], ['of', 'of'], ['of'], ['in', 'of', 'on'], ['of'], ['in'], ['of'], ['in', 'in'], ['of'], ['of', 'of'], ['of', 'of'], ['of', 'in', 'of', 'in', 'of', 'in'], ['for', 'of'], ['of'], ['of', 'on'], ['of', 'of', 'of', 'of'], ['of', 'on', 'of', 'of', 'of'], ['of'], ['of', 'of'], ['of', 'at'], ['of'], ['of'], ['in'], ['of', 'of'], ['on', 'in', 'of', 'of', 'of', 'of'], ['of'], ['of', 'of'], ['in', 'of'], ['of', 'of'], ['in', 'of'], ['on'], ['of'], ['in'], ['of', 'for'], ['for', 'of', 'in', 'of'], ['of'], ['in'], ['in'], ['in', 'in'], ['of', 'of'], ['in'], ['of', 'of'], ['in'], ['of', 'in'], ['of'], ['of'], ['of', 'of'], ['in', 'of', 'of', 'of'], ['of'], ['of'], ['of', 'of'], ['of', 'of'], ['of'], ['in', 'of', 'of'], ['of', 'of', 'for'], ['in', 'of'], ['of'], ['in'], ['of', 'of'], ['in', 'of'], ['in', 'in'], ['of', 'of', 'in'], ['of'], ['in'], ['of', 'of', 'for', 'of', 'of'], ['of', 'in', 'on'], ['of'], ['for', 'in'], ['of', 'on'], ['of'], ['of', 'in', 'of'], ['in'], ['of'], ['of', 'of'], ['in'], ['for', 'of'], ['of'], ['of'], ['of', 'for', 'for'], ['of'], ['in'], ['of', 'of', 'of', 'of', 'of'], ['at', 'of'], ['of', 'of', 'of'], ['in'], ['in'], ['of'], ['of', 'in'], ['of'], ['of'], ['of', 'for'], ['of', 'of', 'of'], ['for', 'in'], ['of'], ['of'], ['of', 'of'], ['in', 'of', 'of', 'of', 'of'], ['for'], ['in', 'of', 'in', 'of'], ['of', 'of'], ['in'], ['of', 'of'], ['in', 'of'], ['on', 'for'], ['of'], ['in', 'of'], ['of', 'of'], ['of', 'of'], ['for', 'for', 'of'], ['of', 'of'], ['for'], ['of', 'of', 'for', 'of', 'of', 'of', 'of', 'of'], ['of'], ['of', 'in'], ['of'], ['of', 'for'], ['for', 'on'], ['of'], ['of', 'of'], ['of', 'of'], ['of']]\n",
            "1429\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I predict the prepositions mainly based on Linear Interpolation n-gram model. We first use nltk.ngrams to seperate the sentence, and when there is a preposition in the three word list, we use the Linear Interpolation n-gram model to calculate the possiblity of each prepostion. And the preposition with the highest possibility in the Linear Interpolation n-gram model will be the output of our prediction. And if the model think every prepostion word possibilty is 0, we will use the bigram model to predict the possibility. And if every prepoistion's possibility is still 0, we will use unigram model to predict it.\n",
        "\n",
        "The accuracy of our model on dev set is 0.6462041409371595.\n",
        "\n",
        "The hyperparameters of the model is same as the values in section 1. We use grid search method to find the best hyperparameters. And that is around 0.8,0.1,0.1.\n",
        "\n",
        "About test perplexitymodel and performance in application, we find the model with lower perplexity will have better performance in applications.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pLUlgAkqGedH"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}